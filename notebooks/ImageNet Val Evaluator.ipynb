{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import timm\n",
    "import timm.models.vision_transformer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_names = ['vit_base_patch16_224', \n",
    "               'vit_base_patch16_224_miil', \n",
    "               'vit_base_patch32_224', \n",
    "               'vit_large_patch16_224']\n",
    "models = {\n",
    "    model_name: timm.create_model(model_name, pretrained=True).eval().to(device)\n",
    "    for model_name in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3951d287deb493dac1e3e013e04d549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 351\u001b[0m\n\u001b[0;32m    345\u001b[0m     model_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    346\u001b[0m         model_name: model_sums[model_name] \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names\n\u001b[0;32m    348\u001b[0m     }\n\u001b[0;32m    350\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgnet_val.db\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 351\u001b[0m \u001b[43mcreate_pred_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 189\u001b[0m, in \u001b[0;36mcreate_pred_db\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    185\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, extractor \u001b[38;5;129;01min\u001b[39;00m extractors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 189\u001b[0m     extraction \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     results \u001b[38;5;241m=\u001b[39m extraction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    193\u001b[0m     model_sums[model_name] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    194\u001b[0m         results, \u001b[38;5;241m1\u001b[39m, labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32me:\\Annaconda\\envs\\vit-interpretability-thesis\\lib\\site-packages\\torch\\fx\\graph_module.py:662\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Annaconda\\envs\\vit-interpretability-thesis\\lib\\site-packages\\torch\\fx\\graph_module.py:271\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[1;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls, obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[1;32me:\\Annaconda\\envs\\vit-interpretability-thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m<eval_with_key>.29:123\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    121\u001b[0m blocks_2_norm2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mnorm2(add_5)\n\u001b[0;32m    122\u001b[0m blocks_2_mlp_fc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mfc1(blocks_2_norm2);  blocks_2_norm2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m blocks_2_mlp_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks_2_mlp_fc1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m blocks_2_mlp_drop1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mdrop1(blocks_2_mlp_act);  blocks_2_mlp_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    125\u001b[0m blocks_2_mlp_fc2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mfc2(blocks_2_mlp_drop1);  blocks_2_mlp_drop1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Annaconda\\envs\\vit-interpretability-thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Annaconda\\envs\\vit-interpretability-thesis\\lib\\site-packages\\torch\\nn\\modules\\activation.py:685\u001b[0m, in \u001b[0;36mGELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from genericpath import isfile\n",
    "from src.datasets.ImageNet import ImageNetValDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.transformation import get_transforms\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "import os\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from src.utils.extraction import extract_value_vectors\n",
    "from src.utils.model import embedding_projection\n",
    "from src.analyzers.vector_analyzer import k_most_predictive_ind_for_classes\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "k_most_pred_by_model = {\n",
    "    model_name: k_most_predictive_ind_for_classes(\n",
    "        embedding_projection(models[model_name], extract_value_vectors(models[model_name], device), device),\n",
    "        10, device)\n",
    "    for model_name in model_names\n",
    "}\n",
    "\n",
    "mlp_fc2_biases = {\n",
    "    model_name: [models[model_name].blocks[i].mlp.fc2.bias.detach() \n",
    "                 for i in range(len(models[model_name].blocks))]\n",
    "    for model_name in model_names\n",
    "}\n",
    "mlp_fc2_biases_l2 = {\n",
    "    model_name: [(mlp_fc2_biases[model_name][i] ** 2).sum().sqrt().item() \n",
    "                 for i in range(len(mlp_fc2_biases[model_name]))]\n",
    "    for model_name in model_names\n",
    "}\n",
    "mlp_fc2_weights = {\n",
    "    model_name: [models[model_name].blocks[i].mlp.fc2.weight.T.detach()\n",
    "                 for i in range(len(models[model_name].blocks))]\n",
    "    for model_name in model_names\n",
    "}\n",
    "model_heads = {\n",
    "    model_name: models[model_name].head.eval()\n",
    "    for model_name in model_names\n",
    "}\n",
    "mlp_fc2_biases_pred = {\n",
    "    model_name: [model_heads[model_name](bias) for bias in mlp_fc2_biases[model_name]]\n",
    "    for model_name in model_names\n",
    "}\n",
    "\n",
    "sqlite3.register_adapter(np.int64, int)\n",
    "sqlite3.register_adapter(np.int32, int)\n",
    "\n",
    "def create_pred_db(path):\n",
    "\n",
    "    transforms = get_transforms(models[model_names[0]])\n",
    "\n",
    "    dataset = ImageNetValDataset('A:\\\\CVData\\\\ImageNet', transforms=transforms)\n",
    "    batch_size = 15\n",
    "    k = 10\n",
    "\n",
    "    connection = sqlite3.connect(path)\n",
    "    cursor = connection.cursor()\n",
    "    connection.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS predictions (\n",
    "            path TEXT,\n",
    "            pred_model TEXT,\n",
    "            imagenet_id TEXT,\n",
    "            num_idx INTEGER,\n",
    "            name TEXT,\n",
    "            {','.join([f'top_{i}_score REAL' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_ind INTEGER' for i in range(k)])},\n",
    "            sum_row REAL,\n",
    "            l1_row REAL,\n",
    "            l2_row REAL,\n",
    "            exp_reciprocal REAL,\n",
    "            min_row REAL\n",
    "        )\n",
    "    \"\"\")\n",
    "    connection.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS vec_activations (\n",
    "            path TEXT,\n",
    "            pred_model TEXT,\n",
    "            imagenet_id TEXT,\n",
    "            num_idx INTEGER,\n",
    "            name TEXT,\n",
    "            {','.join([f'top_{i}_cls_token_act REAL' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_img_token_avg_act REAL' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_block_ind INTEGER' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_vec_ind INTEGER' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_max_cls_token_act REAL' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_max_cls_act_block_ind INTEGER' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_max_cls_act_vec_ind INTEGER' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_max_img_token_avg_act REAL' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_max_img_avg_act_block_ind INTEGER' for i in range(k)])},\n",
    "            {','.join([f'top_{i}_max_img_avg_act_vec_ind INTEGER' for i in range(k)])},\n",
    "            top_bias_l2 REAL,\n",
    "            top_bias_pl_res_l2 REAL,\n",
    "            top_bias_pl_vec_noise_l2 REAL,\n",
    "            top_bias_pl_all_l2 REAL,\n",
    "            path_bias_pl_res_pred TEXT,\n",
    "            path_bias_pl_vec_noise_pred TEXT,\n",
    "            path_bias_pl_all_pred TEXT,\n",
    "            mean_bias_pl_res_pred REAL,\n",
    "            mean_bias_pl_vec_noise_pred REAL,\n",
    "            mean_bias_pl_all_pred REAL,\n",
    "            std_bias_pl_res_pred REAL,\n",
    "            std_bias_pl_vec_noise_pred REAL,\n",
    "            std_bias_pl_all_pred REAL,\n",
    "            max_bias_pl_res_pred REAL,\n",
    "            max_bias_pl_vec_noise_pred REAL,\n",
    "            max_bias_pl_all_pred REAL,\n",
    "            mean_top_bias_pred REAL,\n",
    "            std_top_bias_pred REAL,\n",
    "            max_top_bias_pred REAL,\n",
    "            mean_resid_pred REAL,\n",
    "            std_resid_pred REAL,\n",
    "            max_resid_pred REAL,\n",
    "            mean_vec_noise_pred REAL,\n",
    "            std_vec_noise_pred REAL,\n",
    "            max_vec_noise_pred REAL,\n",
    "            mean_all_pred REAL,\n",
    "            std_all_pred REAL,\n",
    "            max_all_pred REAL\n",
    "        )\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    bias_pl_resid_path = 'A:\\\\CVData\\\\ImageNet\\\\val_top_bias_pl_resid'\n",
    "    bias_pl_vec_noise_path = 'A:\\\\CVData\\\\ImageNet\\\\val_top_bias_pl_vec_noise'\n",
    "    bias_pl_all_path = 'A:\\\\CVData\\\\ImageNet\\\\val_top_bias_pl_all'\n",
    "\n",
    "    model_sums = {\n",
    "        model_name: 0\n",
    "        for model_name in model_names\n",
    "    }\n",
    "    total = len(dataset)\n",
    "\n",
    "    for category in dataset.get_imagenet_classes():\n",
    "        os.makedirs(os.path.join(bias_pl_resid_path, category), exist_ok=True)\n",
    "        os.makedirs(os.path.join(bias_pl_vec_noise_path, category), exist_ok=True)\n",
    "        os.makedirs(os.path.join(bias_pl_all_path, category), exist_ok=True)\n",
    "\n",
    "    extractor_layers = {\n",
    "        model_name: ['head']\n",
    "        for model_name in model_names\n",
    "    }\n",
    "    bias_resid_hooks_handles = {\n",
    "        model_name: []\n",
    "        for model_name in model_names\n",
    "    }\n",
    "\n",
    "    residuals = {\n",
    "        model_name: {}\n",
    "        for model_name in model_names\n",
    "    }\n",
    "\n",
    "    def get_resid_hook(model: str, layer: str):\n",
    "        def hook(module, input, output):\n",
    "            residuals[model][layer] = input[0].detach()\n",
    "        return hook\n",
    "\n",
    "    for model_name, pred_inds in k_most_pred_by_model.items():\n",
    "        # block_counts = pred_inds[:,0,:].flatten().bincount(minlength=len(models[model_name].blocks))\n",
    "        block_counts_top1 = pred_inds[0,0,:].flatten().bincount(minlength=len(models[model_name].blocks))\n",
    "        for i in range(len(models[model_name].blocks)):\n",
    "            extractor_layers[model_name].append(f'blocks.{i}.mlp.fc1')\n",
    "            # if block_counts[i].item() > 0:\n",
    "            if block_counts_top1[i].item() > 0:\n",
    "                handle = models[model_name].blocks[i].norm2.register_forward_hook(\n",
    "                    get_resid_hook(model_name, f'blocks.{i}.norm2'))\n",
    "                bias_resid_hooks_handles[model_name].append(handle)\n",
    "\n",
    "    extractors = {\n",
    "        model_name: create_feature_extractor(models[model_name], extractor_layers[model_name])\n",
    "        for model_name in model_names\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc='Batches'):\n",
    "        \n",
    "        length = min(len(dataset) - i, batch_size)\n",
    "        items = [dataset[i+ii] for ii in range(length)]\n",
    "\n",
    "        total += batch_size\n",
    "\n",
    "        tensor_images = torch.stack([item['img'] for item in items], dim=0).to(device)\n",
    "\n",
    "        labels = torch.tensor([item['num_idx'] for item in items]).to(device)\n",
    "\n",
    "        for model_name, extractor in extractors.items():\n",
    "\n",
    "            extraction = extractor(tensor_images)\n",
    "\n",
    "            results = extraction['head']\n",
    "\n",
    "            model_sums[model_name] += torch.gather(\n",
    "                results, 1, labels.unsqueeze(1)).squeeze().sum().item()\n",
    "            pred_vals, pred_inds = results.topk(k=k, dim=1)\n",
    "\n",
    "            sums = results.sum(dim=1)\n",
    "            l1_rows = results.abs().sum(dim=1)\n",
    "            l2_rows = (results ** 2).sum(dim=1).sqrt()\n",
    "            min_scores = results.min(dim=1)[0]\n",
    "            exp_reciprocals = 1 / results.exp().sum(dim=1)\n",
    "\n",
    "            key_activations_batch = torch.stack([extraction[f'blocks.{i}.mlp.fc1'] \n",
    "                                           for i in range(len(models[model_name].blocks))],\n",
    "                                           dim=0)\n",
    "\n",
    "            rows_pred = []\n",
    "            rows_activations = []\n",
    "            head = model_heads[model_name]\n",
    "            for ii, item in enumerate(items):\n",
    "\n",
    "                rows_pred.append((item['path'], model_name, item['imagenet_id'], \n",
    "                                  item['num_idx'], item['name'], sums[ii].item(), \n",
    "                                  l1_rows[ii].item(), l2_rows[ii].item(), \n",
    "                                  exp_reciprocals[ii].item(), min_scores[ii].item(),\n",
    "                                  *[pred_vals[ii, iii].item() for iii in range(k)],\n",
    "                                  *[pred_inds[ii, iii].item() for iii in range(k)]))\n",
    "                \n",
    "                k_most_pred = k_most_pred_by_model[model_name]\n",
    "                idx = item['num_idx']\n",
    "                key_activations = key_activations_batch[:,ii,:,:]\n",
    "\n",
    "                top_i_cls_token_act = key_activations[k_most_pred[:,0,idx], 0, k_most_pred[:,1,idx]]\n",
    "                top_i_img_token_avg_act = key_activations[k_most_pred[:,0,idx], 1:, k_most_pred[:,1,idx]].mean(dim=1)\n",
    "\n",
    "                blocks, tokens, hidden = key_activations.shape\n",
    "\n",
    "                top_i_max_cls_token_act, top_i_max_cls_token_act_inds = (\n",
    "                    key_activations[:,0,:].flatten().topk(k=k))\n",
    "                \n",
    "                top_i_max_img_token_avg_act, top_i_max_img_token_avg_act_inds = (\n",
    "                    key_activations[:,1:,:].mean(dim=1).flatten().topk(k=k))\n",
    "\n",
    "                top_i_max_cls_token_act_block_inds = top_i_max_cls_token_act_inds // hidden\n",
    "                top_i_max_cls_token_act_col_inds = top_i_max_cls_token_act_inds % hidden\n",
    "\n",
    "                top_i_max_img_token_avg_act_block_inds = top_i_max_img_token_avg_act_inds // hidden\n",
    "                top_i_max_img_token_avg_act_col_inds = top_i_max_img_token_avg_act_inds % hidden\n",
    "\n",
    "                fc2_top_weight = mlp_fc2_weights[model_name][k_most_pred[0,0,idx].item()]\n",
    "                fc2_top_bias = mlp_fc2_biases[model_name][k_most_pred[0,0,idx].item()]\n",
    "                top_bias_pred = mlp_fc2_biases_pred[model_name][k_most_pred[0,0,idx].item()]\n",
    "\n",
    "                cls_residual = residuals[model_name][f'blocks.{k_most_pred[0,0,idx].item()}.norm2']\n",
    "                vec_noise = F.gelu(torch.concat([key_activations[k_most_pred[0,0,idx], 0, :k_most_pred[0,1,idx]],\n",
    "                                          key_activations[k_most_pred[0,0,idx], 0, k_most_pred[0,1,idx]+1:]],\n",
    "                                          dim=-1)) @ torch.concat([fc2_top_weight[:k_most_pred[0,1,idx],:],\n",
    "                                                                   fc2_top_weight[k_most_pred[0,1,idx]+1:]],\n",
    "                                                                   dim=0)\n",
    "                \n",
    "                resid_pred = head(cls_residual)\n",
    "                top_bias_pl_res_l2 = cls_residual + fc2_top_bias\n",
    "                top_bias_pl_res_pred = head(top_bias_pl_res_l2)\n",
    "                top_bias_pl_res_l2 = (top_bias_pl_res_l2 ** 2).sum().sqrt().item()\n",
    "\n",
    "                vec_noise_pred = head(vec_noise)\n",
    "                top_bias_pl_vec_noise_l2 = vec_noise + fc2_top_bias\n",
    "                top_bias_pl_vec_noise_pred = head(top_bias_pl_vec_noise_l2)\n",
    "                top_bias_pl_vec_noise_l2 = (top_bias_pl_vec_noise_l2 ** 2).sum().sqrt().item()\n",
    "\n",
    "                all_pred = head(vec_noise + cls_residual)\n",
    "                top_bias_pl_all_l2 = cls_residual + vec_noise + fc2_top_bias\n",
    "                top_bias_pl_all_pred = head(top_bias_pl_all_l2)\n",
    "                top_bias_pl_all_l2 = (top_bias_pl_all_l2 ** 2).sum().sqrt().item()\n",
    "\n",
    "                top_bias_pl_res_pred_path = os.path.join(bias_pl_resid_path, item['imagenet_id'], Path(item['path']).stem) + '.pt'\n",
    "                top_bias_pl_vec_noise_pred_path = os.path.join(bias_pl_vec_noise_path, item['imagenet_id'], Path(item['path']).stem) + '.pt'\n",
    "                top_bias_pl_all_pred_path = os.path.join(bias_pl_all_path, item['imagenet_id'], Path(item['path']).stem) + '.pt'\n",
    "\n",
    "                torch.save(top_bias_pl_res_pred, top_bias_pl_res_pred_path)\n",
    "                torch.save(top_bias_pl_vec_noise_pred, top_bias_pl_vec_noise_pred_path)\n",
    "                torch.save(top_bias_pl_all_pred, top_bias_pl_all_pred_path)\n",
    "\n",
    "                rows_activations.append((\n",
    "                    item['path'], model_name, item['imagenet_id'], item['num_idx'], item['name'],\n",
    "                    mlp_fc2_biases_l2[model_name][k_most_pred[0,0,idx].item()], top_bias_pl_res_l2,\n",
    "                    top_bias_pl_vec_noise_l2, top_bias_pl_all_l2, top_bias_pl_res_pred_path,\n",
    "                    top_bias_pl_vec_noise_pred_path, top_bias_pl_all_pred_path, top_bias_pl_res_pred.mean().item(),\n",
    "                    top_bias_pl_vec_noise_pred.mean().item(), top_bias_pl_all_pred.mean().item(),\n",
    "                    top_bias_pl_res_pred.std().item(), top_bias_pl_vec_noise_pred.std().item(),\n",
    "                    top_bias_pl_all_pred.std().item(), top_bias_pred.mean().item(), top_bias_pred.std().item(),\n",
    "                    resid_pred.mean().item(), resid_pred.std().item(), vec_noise_pred.mean().item(), \n",
    "                    vec_noise_pred.std().item(), all_pred.mean().item(), all_pred.std().item(),\n",
    "                    top_bias_pl_res_pred.max().item(), top_bias_pl_vec_noise_pred.max().item(),\n",
    "                    top_bias_pl_all_pred.max().item(), top_bias_pred.max().item(), resid_pred.max().item(),\n",
    "                    vec_noise_pred.max().item(), all_pred.max().item(),\n",
    "                    *[top_i_cls_token_act[iii].item() for iii in range(k)],\n",
    "                    *[top_i_img_token_avg_act[iii].item() for iii in range(k)],\n",
    "                    *[k_most_pred[iii,0,idx].item() for iii in range(k)],\n",
    "                    *[k_most_pred[iii,1,idx].item() for iii in range(k)],\n",
    "                    *[top_i_max_cls_token_act[iii].item() for iii in range(k)],\n",
    "                    *[top_i_max_cls_token_act_block_inds[iii].item() for iii in range(k)],\n",
    "                    *[top_i_max_cls_token_act_col_inds[iii].item() for iii in range(k)],\n",
    "                    *[top_i_max_img_token_avg_act[iii].item() for iii in range(k)],\n",
    "                    *[top_i_max_img_token_avg_act_block_inds[iii].item() for iii in range(k)],\n",
    "                    *[top_i_max_img_token_avg_act_col_inds[iii].item() for iii in range(k)]\n",
    "                ))\n",
    "                \n",
    "\n",
    "            cursor.executemany(f\"\"\"\n",
    "                    INSERT INTO vec_activations (path, pred_model, imagenet_id, num_idx, name, top_bias_l2,\n",
    "                    top_bias_pl_res_l2, top_bias_pl_vec_noise_l2, top_bias_pl_all_l2, path_bias_pl_res_pred,\n",
    "                    path_bias_pl_vec_noise_pred, path_bias_pl_all_pred, mean_bias_pl_res_pred, \n",
    "                    mean_bias_pl_vec_noise_pred, mean_bias_pl_all_pred, std_bias_pl_res_pred,\n",
    "                    std_bias_pl_vec_noise_pred, std_bias_pl_all_pred, mean_top_bias_pred, std_top_bias_pred,\n",
    "                    mean_resid_pred, std_resid_pred, mean_vec_noise_pred, std_vec_noise_pred,\n",
    "                    mean_all_pred, std_all_pred, \n",
    "                    max_bias_pl_res_pred, max_bias_pl_vec_noise_pred, \n",
    "                    max_bias_pl_all_pred, max_top_bias_pred, max_resid_pred, max_vec_noise_pred, max_all_pred,\n",
    "                    {','.join([f'top_{iii}_cls_token_act' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_img_token_avg_act' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_block_ind' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_vec_ind' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_max_cls_token_act' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_max_cls_act_block_ind' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_max_cls_act_vec_ind' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_max_img_token_avg_act' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_max_img_avg_act_block_ind' for iii in range(k)])},\n",
    "                    {','.join([f'top_{iii}_max_img_avg_act_vec_ind' for iii in range(k)])}) VALUES\n",
    "                    (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,\n",
    "                    {','.join(['?' for _ in range(10*k)])})\n",
    "                \"\"\", rows_activations\n",
    "            )\n",
    "            cursor.executemany(f\"\"\"\n",
    "                    INSERT INTO predictions (path, pred_model, imagenet_id, num_idx, name,\n",
    "                        sum_row, l1_row, l2_row, exp_reciprocal, min_row, \n",
    "                        {\",\".join([f'top_{iii}_score' for iii in range(k)])},\n",
    "                        {\",\".join([f'top_{iii}_ind' for iii in range(k)])}) VALUES \n",
    "                    (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, {\", \".join([\"?\" for _ in range(2*k)])})\n",
    "                \"\"\", rows_pred\n",
    "            )\n",
    "        connection.commit()\n",
    "        for item in items:\n",
    "            del item['img']\n",
    "        del tensor_images\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    for model_name in model_names:\n",
    "        for handle in bias_resid_hooks_handles[model_name]:\n",
    "            handle.remove()\n",
    "\n",
    "    model_scores = {\n",
    "        model_name: model_sums[model_name] / total\n",
    "        for model_name in model_names\n",
    "    }\n",
    "\n",
    "path = 'imgnet_val.db'\n",
    "create_pred_db(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-interpretability-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
